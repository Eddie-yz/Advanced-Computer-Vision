{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# CSE252C: Homework 1\n",
    "## Computing Resources\n",
    "Please read the README file of this repository for the instructions\n",
    "## Instructions\n",
    "1. Attempt all questions.\n",
    "2. Please comment all your code adequately.\n",
    "3. Include all relevant information such as text answers, output images in notebook.\n",
    "4. **Academic integrity:** The homework must be completed individually.\n",
    "\n",
    "5. **Submission instructions:**  \n",
    " (a) Submit the notebook and its PDF version on Gradescope.  \n",
    " (b) Rename your submission files as Lastname_Firstname.ipynb and Lastname_Firstname.pdf.  \n",
    " (c) Correctly select pages for each answer on Gradescope to allow proper grading.\n",
    "\n",
    "6. **Due date:** Assignments are due Mon, May 4, by 4pm PST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Question 1: Warm Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We will first try SFM using the original implementation from $\\mathtt{libviso2}$[8],[9]. We will test on a dataset containing 300 images from one sequence of the KITTI dataset with ground-truth camera poses and camera calibration information. \n",
    "\n",
    "Run the SFM algorithm using the following script. You are required to report two error metrics. The error metric for rotation is defined as the mean of Frobenius norm of the difference between the ground-truth rotation matrix and predicted rotation matrix. The error metric for translation is defined as mean of the L2 distance. Both errors will be printed on the screen as you run the code.  **(5 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/datasets/home/home-02/59/959/yiz086/cse252c_hw1/pyviso\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "# change your base path\n",
    "os.chdir('./pyviso/') # './'\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4)\n",
    "np.set_printoptions(suppress=True)\n",
    "import viso2\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread\n",
    "import time\n",
    "\n",
    "def errorMetric(RPred, RGt, TPred, TGt):\n",
    "    diffRot = (RPred - RGt)\n",
    "    diffTrans = (TPred - TGt)\n",
    "    errorRot = np.sqrt(np.sum(np.multiply(diffRot.reshape(-1), diffRot.reshape(-1))))\n",
    "    errorTrans = np.sqrt(np.sum(np.multiply(diffTrans.reshape(-1), diffTrans.reshape(-1))))\n",
    "\n",
    "    return errorRot, errorTrans\n",
    "\n",
    "if_vis = True # set to True to do the visualization per frame; the images will be saved at '.vis/'. Turn off if you just want the camera poses and errors\n",
    "if_on_screen = False # if True the visualization per frame is going to be displayed realtime on screen; if False there will be no display, but in both options the images will be saved\n",
    "\n",
    "# parameter settings (for an example, please download\n",
    "# dataset_path = '../dataset'\n",
    "dataset_path = '/datasets/cse152-252-sp20-public/dataset_SfM' # On the ``ieng6.ucsd.edu`` server\n",
    "img_dir      = os.path.join(dataset_path, 'sequences/00/image_0')\n",
    "gt_dir       = os.path.join(dataset_path, 'poses/00.txt')\n",
    "calibFile    = os.path.join(dataset_path, 'sequences/00/calib.txt')\n",
    "border       = 50;\n",
    "gap          = 15;\n",
    "\n",
    "# Load the camera calibration information\n",
    "with open(calibFile) as fid:\n",
    "    calibLines = fid.readlines()\n",
    "    calibLines = [calibLine.strip() for calibLine in calibLines]\n",
    "\n",
    "calibInfo = [float(calibStr) for calibStr in calibLines[0].split(' ')[1:]]\n",
    "# param = {'f': calibInfo[0], 'cu': calibInfo[2], 'cv': calibInfo[6]}\n",
    "\n",
    "# Load the ground-truth depth and rotation\n",
    "with open(gt_dir) as fid:\n",
    "    gtTr = [[float(TrStr) for TrStr in line.strip().split(' ')] for line in fid.readlines()]\n",
    "gtTr = np.asarray(gtTr).reshape(-1, 3, 4)\n",
    "\n",
    "# param['height'] = 1.6\n",
    "# param['pitch']  = -0.08\n",
    "# param['match'] = {'pre_step_size': 64}\n",
    "first_frame  = 0\n",
    "last_frame   = 300\n",
    "epi = 1e-8\n",
    "\n",
    "# init visual odometry\n",
    "params = viso2.Mono_parameters()\n",
    "params.calib.f = calibInfo[0]\n",
    "params.calib.cu = calibInfo[2]\n",
    "params.calib.cv = calibInfo[6]\n",
    "params.height = 1.6\n",
    "params.pitch = -0.08\n",
    "\n",
    "\n",
    "first_frame  = 0\n",
    "last_frame   = 300\n",
    "\n",
    "# init transformation matrix array\n",
    "Tr_total = []\n",
    "Tr_total_np = []\n",
    "Tr_total.append(viso2.Matrix_eye(4))\n",
    "Tr_total_np.append(np.eye(4))\n",
    "\n",
    "# init viso module\n",
    "visoMono = viso2.VisualOdometryMono(params)\n",
    "\n",
    "if if_vis:\n",
    "    save_path = 'vis'\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    # create figure\n",
    "    fig = plt.figure(figsize=(10, 15))\n",
    "    ax1 = plt.subplot(211)\n",
    "    ax1.axis('off')\n",
    "    ax2 = plt.subplot(212)\n",
    "    ax2.set_xticks(np.arange(-100, 100, step=10))\n",
    "    ax2.set_yticks(np.arange(-500, 500, step=10))\n",
    "    ax2.axis('equal')\n",
    "    ax2.grid()\n",
    "    if if_on_screen:\n",
    "        plt.ion()\n",
    "    else:\n",
    "        plt.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# for all frames do\n",
    "if_replace = False\n",
    "errorTransSum = 0\n",
    "errorRotSum = 0\n",
    "errorRot_list = []\n",
    "errorTrans_list =[]\n",
    "\n",
    "for frame in range(first_frame, last_frame):\n",
    "    # 1-based index\n",
    "    k = frame-first_frame+1\n",
    "\n",
    "    # read current images\n",
    "    I = imread(os.path.join(img_dir, '%06d.png'%frame))\n",
    "    assert(len(I.shape) == 2) # should be grayscale\n",
    "\n",
    "    # compute egomotion\n",
    "    process_result = visoMono.process_frame(I, if_replace)\n",
    "    Tr = visoMono.getMotion()\n",
    "    matrixer = viso2.Matrix(Tr)\n",
    "    Tr_np = np.zeros((4, 4))\n",
    "    Tr.toNumpy(Tr_np) # so awkward...\n",
    "\n",
    "    # accumulate egomotion, starting with second frame\n",
    "    if k > 1:\n",
    "        if process_result is False:\n",
    "            if_replace = True\n",
    "            Tr_total.append(Tr_total[-1])\n",
    "            Tr_total_np.append(Tr_total_np[-1])\n",
    "        else:\n",
    "            if_replace = False\n",
    "            Tr_total.append(Tr_total[-1] * viso2.Matrix_inv(Tr))\n",
    "            Tr_total_np.append(Tr_total_np[-1] @ np.linalg.inv(Tr_np)) # should be the same\n",
    "            print(Tr_total_np[-1])\n",
    "\n",
    "    # output statistics\n",
    "    num_matches = visoMono.getNumberOfMatches()\n",
    "    num_inliers = visoMono.getNumberOfInliers()\n",
    "    matches = visoMono.getMatches()\n",
    "    matches_np = np.empty([4, matches.size()])\n",
    "\n",
    "    for i,m in enumerate(matches):\n",
    "        matches_np[:, i] = (m.u1p, m.v1p, m.u1c, m.v1c)\n",
    "\n",
    "    if if_vis:\n",
    "        # update image\n",
    "        ax1.clear()\n",
    "        ax1.imshow(I, cmap='gray', vmin=0, vmax=255)\n",
    "        if num_matches != 0:\n",
    "            for n in range(num_matches):\n",
    "                ax1.plot([matches_np[0, n], matches_np[2, n]], [matches_np[1, n], matches_np[3, n]])\n",
    "        ax1.set_title('Frame %d'%frame)\n",
    "\n",
    "        # update trajectory\n",
    "        if k > 1:\n",
    "            ax2.plot([Tr_total_np[k-2][0, 3], Tr_total_np[k-1][0, 3]], \\\n",
    "                [Tr_total_np[k-2][2, 3], Tr_total_np[k-1][2, 3]], 'b.-', linewidth=1)\n",
    "            ax2.plot([gtTr[k-2][0, 3], gtTr[k-1][0, 3]], \\\n",
    "                [gtTr[k-2][2, 3], gtTr[k-1][2, 3]], 'r.-', linewidth=1)\n",
    "        ax2.set_title('Blue: estimated trajectory; Red: ground truth trejectory')\n",
    "\n",
    "        plt.draw()\n",
    "\n",
    "    # Compute rotation\n",
    "    Rpred_p = Tr_total_np[k-2][0:3, 0:3]\n",
    "    Rpred_c = Tr_total_np[k-1][0:3, 0:3]\n",
    "    Rpred = Rpred_c.transpose() @ Rpred_p\n",
    "    Rgt_p = np.squeeze(gtTr[k-2, 0:3, 0:3])\n",
    "    Rgt_c = np.squeeze(gtTr[k-1, 0:3, 0:3])\n",
    "    Rgt = Rgt_c.transpose() @ Rgt_p\n",
    "    # Compute translation\n",
    "    Tpred_p = Tr_total_np[k-2][0:3, 3:4]\n",
    "    Tpred_c = Tr_total_np[k-1][0:3, 3:4]\n",
    "    Tpred = Tpred_c - Tpred_p\n",
    "    Tgt_p = gtTr[k-2, 0:3, 3:4]\n",
    "    Tgt_c = gtTr[k-1, 0:3, 3:4]\n",
    "    Tgt = Tgt_c - Tgt_p\n",
    "    # Compute errors\n",
    "    errorRot, errorTrans = errorMetric(Rpred, Rgt, Tpred, Tgt)\n",
    "    errorRotSum = errorRotSum + errorRot\n",
    "    errorTransSum = errorTransSum + errorTrans\n",
    "    # errorRot_list.append(errorRot)\n",
    "    # errorTrans_list.append(errorTrans)\n",
    "    print('Mean Error Rotation: %.5f'%(errorRotSum / (k-1+epi)))\n",
    "    print('Mean Error Translation: %.5f'%(errorTransSum / (k-1+epi)))\n",
    "\n",
    "\n",
    "\n",
    "    print('== [Result] Frame: %d, Matches %d, Inliers: %.2f'%(frame, num_matches, 100*num_inliers/(num_matches+1e-8)))\n",
    "\n",
    "    if if_vis:\n",
    "        # input('Paused; Press Enter to continue') # Option 1: Manually pause and resume\n",
    "        if if_on_screen:\n",
    "            plt.pause(0.1) # Or Option 2: enable to this to auto pause for a while after daring to enable animation in case of a delay in drawing\n",
    "        vis_path = os.path.join(save_path, 'frame%03d.jpg'%frame)\n",
    "        fig.savefig(vis_path)\n",
    "        print('Saved at %s'%vis_path)\n",
    "        \n",
    "        if frame % 50 == 0 or frame == last_frame-1:\n",
    "            plt.figure(figsize=(10, 15))\n",
    "            plt.imshow(plt.imread(vis_path))\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "# input('Press Enter to exit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "1. Report the final rotation and translation error. **(2 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Mean Error Rotation: 0.00277\n",
    "<br>\n",
    "Mean Error Translation: 0.53721"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Then answer the questions below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "2. In $\\mathtt{libviso2}$, the feature points are \"bucketed\" ($\\mathtt{src/matcher.cpp: Line 285 - 326}$), which means in a certain area of region, the number of detected keypoint pairs should be within certain bounds. Why?  **(3 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "(a). Reducing the number of features can reduce the computation of the algorithm, which is important for real time application<br>\n",
    "(b). Bucketing the features can ensure the used features are well distributed along the z-axis, which is important for estimating linear and angular speed.<br>\n",
    "(c). Avoiding that most of the extracted image features are lie on the moving objects. Uniformly distrubuted features guarantee that the features from static background are used to estimate egomotion. And this can reduce the drift rates of the approach. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "3. We have run SFM on a single camera, which means the scale of translation is unknown. However, as you may have observed, the predicted trajectory is still somehow similar to the ground-truth trajectory. How does $\\mathtt{libviso2}$ handle this ambiguity ($\\mathtt{viso\\_mono.cpp: Line 245}$)?  **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "It assumes that the camera is at a fixed and known height over the ground all the time. A 1-point RANSAC is used to find a ground plane to 3D points. Then we can scale the translation by making the estimated camera height match the known height of the car."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "4. Briefly explain the RANSAC algorithm used in $\\mathtt{libviso2}$ ($\\mathtt{viso\\_mono.cpp: Line 113 - 129}$).  **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Iterate the steps below for ransac_iters times: <br>\n",
    "- Sample 8 matched pairs from all matched feature points randomly\n",
    "- Compute matrix F using this 8 point pairs\n",
    "- Count the number of inliers by the estimated fundamental matrix F\n",
    "- If the number is larger than the previous best result, then update the inlier-matchers and the number\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Question 2: Using SIFT [4] for SFM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In the second task, you are required to use keypoints and feature descriptors from SIFT for SFM. The SIFT implementation can be found in directory $\\mathtt{SIFT}$. \n",
    "\n",
    "(A) Go to $\\mathtt{SIFT}$ directory and run $\\mathtt{runSIFT.py}$ (e.g. `python runSIFT.py --input /datasets/cse152-252-sp20-public/dataset_SfM/sequences/00/image_0/`). You will save the detected keypoints and feature descriptors under the directory $\\mathtt{SIFT}$. For image $\\mathtt{000abc.png}$, the pre-computed features and keypoints should be saved in a $\\mathtt{.npy}$ file named as $\\mathtt{000abc\\_feature.npy}$. The variable should be a $130 \\times N$ matrix with $\\mathtt{single}$ precision, where $N$ is the number of feature points being detected. For each $130$-dimensional feature vector, the first two dimensions are the location of the keypoints (column number first and then row number) on the image plane and the last $128$ dimensions are the feature descriptor. \n",
    "\n",
    "(B) Run the following script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import runFeature\n",
    "dataset_path = '/datasets/cse152-252-sp20-public/dataset_SfM'\n",
    "feature_dir = 'SIFT'\n",
    "runFeature.runSFM(dataset_path, feature_dir )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "1. Report the final rotation and translation error. **(2 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Mean Error Rotation: 0.00243\n",
    "<br>\n",
    "Mean Error Translation: 0.55841"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Next, answer the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "2. Does SIFT yield higher accuracy than the original $\\mathtt{libviso2}$? Why or why not? If not, can you suggest one possible way to improve? **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "- No, it doesn't achieve higher accuracy than the original libviso2 <br>\n",
    "- From the visualization of SIFT features, we notice that there exist many mismatches. In this street scenario, there are many similar patterns on the ground or roadside windows. These mismatches might hurt the estimation very much.<br>\n",
    "- The original libviso2 detects much more interest points on the ground than SIFT does, which I think is important for the egomotion estimation.\n",
    "- I think one way to improve is increasing the threshold of matching criteria of two feature points. This will lead to less but more accurate point matching.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "3. Explain how SIFT achieves invariance to \n",
    "       a. illumination\n",
    "       b. rotation\n",
    "       c. scale\n",
    " **(3 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "- Illumination: firstly, use gradients instead of the original pixel value, so a constant light won't effect it; secondly, the descriptors are normlized to unit length before output, so the contrast change will be canceled (However, this cannot deal with non-linear illumination changes).\n",
    "- Rotation: A neighbourhood is taken around a keypoint and an orientation histogram is computed in this region. The highest gradient magnitude peak is selected as the major orientation of this keypoint. When computing the descriptor, the gradients are rotated according to this keypoint direction, so the rotation invariance can be achieved.\n",
    "- Scale: LoG is not scale invariant because its response to a edge decreases as the scale $\\sigma$ increase. To get the scale invariance, we neet to multiply the LoG with $\\sigma^2$. In practice, DoG function is used, which already incorporates the scale normalizaiton requirement. So it can detect blobs under different scales and achieve scale invariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Question 3: Using SuperPoint[1] for SFM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now you are required to use keypoints and feature descriptors from SuperPoint for SFM. The code for the trained model of this method can be found from the $\\mathtt{SuperPoint}$.\n",
    "\n",
    "(A) Go to $\\mathtt{SuperPoint}$ directory and run $\\mathtt{demo\\_superpoint.py}$. The detected keypoints and feature descriptors are under the directory $\\mathtt{SuperPoint}$. The file format is similar to the SIFT case. For image $\\mathtt{000abc.png}$, the pre-computed features and keypoints should be saved in a $\\mathtt{.npy}$ file named as $\\mathtt{000abc\\_feature.npy}$. The variable is a $258\\times N$ matrix with $\\mathtt{single}$ precision, where $N$ is the number of feature points being detected. For each $258$-dimensional feature vector, the first two dimensions are the locations of the keypoint (column number first and then row number) on the image plane and the last $256$ dimensions represent the feature descriptor. \n",
    "\n",
    "(B) Run the following script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import runFeature\n",
    "dataset_path = '/datasets/cse152-252-sp20-public/dataset_SfM'\n",
    "feature_dir = 'SuperPoint'\n",
    "runFeature.runSFM(dataset_path, feature_dir )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Report the final rotation and translation error. **(2 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Error Rotation: 0.00369\n",
    "<br>\n",
    "Mean Error Translation: 0.52405"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Next, answer the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "2. Does SuperPoint yield higher accuracy than the original $\\mathtt{libviso2}$? If so, why? If not, what steps can you take to improve? **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Super point gets lower accuracy on the rotation, while performs slightly better on the translation.\n",
    "- SuperPoint is not trained on KITTI dataset, which may explain why it performs bad on these pictures.\n",
    "- The feature descriptors of SuperPoint is obtained by bilinear unsampling, which might hurt the performance.\n",
    "- Maybe the framework of libviso2 is specifically designed for the features extracted by itself?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "3. Explain briefly how the following issues are being handled in SuperPoint: **(3 points)**\n",
    "       a. Obtaining ground truth for keypoints.\n",
    "       b. Cheaply obtaining accurate ground truth matches, as compared to LIDAR in UCN or SFM in LIFT.\n",
    "       c. Learning a correlated feature representation for keypoint detection and description? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "(a). It first pretrained a MagicPoint network on a large synthetic dataset in which interest points are easy to define. Apply this encoder to homographic adapted real-world unlabeled images and then aggregate them to obtain self-labeled interest points.\n",
    "\n",
    "(b). After generating pseudo-ground truth labels, another homography is randomly sampled, and then the image and corresponding pseudo-ground truth are transformed by the homography to create the needed inputs and labels. In this way, we get the ground truth matches.\n",
    "\n",
    "(c). A shared-parameter encoder is used to compute the representations for both keypoint detection and feature descriptor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Question 4: Using SPyNet [5] for SFM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now we will compute camera motion from optical flow computed using SPyNet. We first uniformly sample points in an image, then consider the flow-displaced point in the other image as a match. A modified PyTorch implementation of SPyNet is provided in directory  $\\mathtt{Flow}$.\n",
    "\n",
    "(A) Go to $\\mathtt{Flow}$ and run $\\mathtt{demo\\_spynet.py}$. \n",
    "\n",
    "(B) Run the following script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import runMatch\n",
    "dataset_path = '/datasets/cse152-252-sp20-public/dataset_SfM'\n",
    "feature_dir = 'Flow'\n",
    "runMatch.runSFM(dataset_path, feature_dir )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "1. Report the final rotation and translation error. **(2 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Mean Error Rotation: 0.00380\n",
    "<br>\n",
    "Mean Error Translation: 0.61071"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Next, answer the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "2. Does SPyNet yield higher accuracy than the original $\\mathtt{libviso2}$? Why or why not? If not, what steps can you take to improve? **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "No, SPyNet performs the worst among all the models above.\n",
    "- SpyNet is trying to predict dense matching between frames instead of sparse feature points. However, on the street scenario, a large portion of the image is the flat ground, which is difficult to find good dense matching pairs on. I think these unaccurate matchings might be the reason why it doesn't perform well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "3. Explain how SPyNet achieves accurate flow with significantly lower computational cost. **(3 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "The goal of SPyNet is to learn a CNN $G_k$ to predict residual flow at each level $k$, this $G_k$ is expected to sequentially predict residual given trained $G_{k-1}$. At each level, it just solve a simple problem, so each $G_k$ can have a simple architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Question 5: Using Sfm-learner for SFM\n",
    "Now we will use deep neural networks for SFM. Please follow the open-source Sfmlearner repository (https://github.com/ClementPinard/SfmLearner-Pytorch) and the paper (https://people.eecs.berkeley.edu/~tinghuiz/projects/SfMLearner/cvpr17_sfm_final.pdf) to answer the following questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "1. Describe how photometric loss is implemented in Sfmlearner? Please refer to the paper and the code. **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "- Select frame $t$ as target image, frame $t-1$ and frame $t+1$ as reference images.\n",
    "- Use $disp\\_net$ to compute depth map of the target image, and use $pose\\_exp\\_net$ to compute relative camera poses between the target image and two reference images.\n",
    "- Use $inverse\\_warp$ to do a depth image-based rendering, which warps the reference images to the target image plane.\n",
    "- Compute the diff between the target image and two rendered images, scale them by the weights of the predicted mask and then sum them up as the photometric loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "2. Train the model and show the training curve, validation curve, and visualization of predicted depth and warped images? Which loss is decreasing and which is increasing? Why? **(10 points)**\n",
    "\n",
    " Visualize the specific images: `val_Depth_Output_Normalized/0`, `val_Warped_Outputs/0` from tensorboard.\n",
    " \n",
    " Training data: \n",
    "`/datasets/cse152-252-sp20-public/sfmlearner_h128w416`.\n",
    "\n",
    "#### Install the environment\n",
    "`pip install -r requirements.txt`\n",
    "- fix scipy version problem: \n",
    "\n",
    "`pip install scipy==1.1.0 --user`\n",
    "\n",
    "#### Training script\n",
    "`cd SfmLearner-Pytorch`\n",
    "\n",
    "`python3 train.py /datasets/cse152-252-sp20-public/sfmlearner_h128w416 -b4 -m0.2 -s0.1 --epoch-size 3000 --sequence-length 3 --log-output`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**The serve keep killing my pods every 4-5 hours even though I have used the K8S_TIMEOUT_SECONDS=43200 flag. So I have to load the pretrained model and continue to train it everytime after the pod has been killed. Eventually, I trained both the original model and the model with new loss added for around 40 epochs. The curve plots below are just from a part of the training.**\n",
    "#### Training Curve:\n",
    "![training_plot](old_train_loss.png)\n",
    "- disparity_smoothness_loss is increasing\n",
    "- explanability_loss and photometric_loss are decreasing\n",
    "\n",
    "#### Validation Curve:\n",
    "![val_loss](old_val_loss.png)\n",
    "- explanability_loss and photometric_loss are decreasing\n",
    "\n",
    "#### Reason:\n",
    "- The depth-net and pose-net are becoming better, so the estimated relative camera poses and the depth maps of target images are becoming more accurate. Therefore, the photometric_error are decreasing.\n",
    "- The ground truth for the explainability mask is 1 at every pixel, so the model encourages to compute photometric loss at as more pixels as possible. During the training, the model is becoming better and the photometric loss is becoming smaller, so the explainability mask is able to have more and more positions assigned with 1 while doesn't increase the total loss too much. As the value of more positions in the mask approaching 1, the explanability_loss is getting smaller.\n",
    "- As the predicted depth map becoming more accurate, the whole image cannot just has the same depth everywhere, so the smoothness is decreasing and loss is increasing.\n",
    "\n",
    "#### Depth map\n",
    "![depth_map](old_depth_map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "3. When training the model, we use 3 consecutive frames. Now, you will use the photometric consistency between the 1st and the 3rd frame. To be more specific, you can get the pose $T_{1,3} = T_{2,3} @ T_{1,2}$, where $T_{1,2}, T_{2,3}$ have already been computed in the original code. Add the constraint to the total loss and report the results in the same manner as in part 2 above. **(10 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "The tendencies are the same with the original model.\n",
    "#### Training Curve:\n",
    "![training_plot](new_train_loss.png)\n",
    "- disparity_smoothness_loss is increasing\n",
    "- explanability_loss and photometric_loss are decreasing\n",
    "\n",
    "#### Validation Curve:\n",
    "- explanability_loss and photometric_loss are decreasing\n",
    "\n",
    "#### Reason:\n",
    "I think adding the new loss doesn't change the tendency of the loss curves. So the reasons are exactly the same with the last question.\n",
    "\n",
    "#### Depth map:\n",
    "![depth_map](new_depth_map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "4. Now, you will evaluate your models from parts (2), (3) on the KITTI odometry dataset. What are the error metrics used in Sfm-learner? Please report the `ATE` and `RE` for sequence `09` and `10`. **(5 points)**\n",
    "\n",
    "  Data: `/datasets/cse152-252-sp20-public/kitti`.\n",
    "\n",
    "#### Evaluation script\n",
    "`python3 test_pose.py /path/to/posenet --dataset-dir /datasets/cse152-252-sp20-public/kitti --sequences 09`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Error Metrics\n",
    "- For depth:\n",
    "    - Abs Relative difference\n",
    "    - Squared Relative difference\n",
    "    - RMSE (linear)\n",
    "    - RMSE (log)\n",
    "- For pose:\n",
    "    - Absolute Trajectory Error (ATE)\n",
    "    - Rotation Error (RE)\n",
    "\n",
    "\n",
    "### For original loss model:\n",
    "|   |Sequence 09   |Sequence 10   |   |   |\n",
    "|---|---|---|---|---|\n",
    "|ATE(mean)   |**0.0100**   |**0.0090**   |   |   |\n",
    "|ATE(std)   |0.0061   |0.0068   |   |   |\n",
    "|RE(mean)   |0.0024   |0.0025   |   |   |\n",
    "|RE(std)   |0.0015   |0.0018   |   |   |\n",
    "\n",
    "###  For new loss model:\n",
    "|   |Sequence 09   |Sequence 10   |   |   |\n",
    "|---|---|---|---|---|\n",
    "|ATE(mean)   |0.0113   |0.0092   |   |   |\n",
    "|ATE(std)   |0.0069   |0.0073   |   |   |\n",
    "|RE(mean)   |**0.0021**   |**0.0022**   |   |   |\n",
    "|RE(std)   |0.0013   |0.0015   |   |   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Notes:\n",
    "- scp data to local machines:\n",
    "\n",
    "`scp -r <USERNAME>@dsmlp-login.ucsd.edu:/datasets/cse152-252-sp20-public/sfmlearner_h128w416.zip`\n",
    "\n",
    "`scp -r <USERNAME>@dsmlp-login.ucsd.edu:/datasets/cse152-252-sp20-public/kitti.zip`\n",
    "...\n",
    "- tensorboard: open jupyter notebook from the link after `launch-scipy-ml-gpu.sh`. Click new `Tensorboard ..`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# References\n",
    "1. Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superpoint: Self-supervised interest point detection and description. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 224–236, 2018.\n",
    "2. Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In Conference on Computer Vision and Pattern Recognition (CVPR), 2012.\n",
    "3. Andreas Geiger, Julius Ziegler, and Christoph Stiller. Stereoscan: Dense 3d reconstruction in real-time. In Intelligent Vehicles Symposium (IV), 2011.\n",
    "4. David G Lowe. Distinctive image features from scale-invariant keypoints. IJCV, 60(2):91–110, 2004.\n",
    "5. Anurag Ranjan and Michael J Black. Optical flow estimation using a spatial pyramid network. In\n",
    "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4161–4170, 2017.\n",
    "6. A. Vedaldi and B. Fulkerson. VLFeat: An open and portable library of computer vision algorithms. http://www.vlfeat.org/, 2008.\n",
    "7. Lucas, Bruce D., and Takeo Kanade. \"An iterative image registration technique with an application to stereo vision.\" (1981): 674.\n",
    "8. B. Kitt, A. Geiger, and H. Lategahn, “Visual odometry based on stereo image sequences with RANSAC-based outlier rejection scheme,” in 2010 IEEE Intelligent Vehicles Symposium, La Jolla, CA, USA, Jun. 2010, pp. 486–492, doi: 10.1109/IVS.2010.5548123.\n",
    "9. A. Geiger, J. Ziegler, and C. Stiller, “StereoScan: Dense 3d reconstruction in real-time,” in 2011 IEEE Intelligent Vehicles Symposium (IV), Baden-Baden, Germany, Jun. 2011, pp. 963–968, doi: 10.1109/IVS.2011.5940405."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "key": "kernelspec",
     "op": "add",
     "value": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     }
    },
    {
     "key": "language_info",
     "op": "add",
     "value": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
     }
    }
   ],
   "remote_diff": [
    {
     "key": "kernelspec",
     "op": "add",
     "value": {
      "display_name": "py36-torch",
      "language": "python",
      "name": "myenv"
     }
    },
    {
     "key": "language_info",
     "op": "add",
     "value": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
     }
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
